# 简单爬虫防范

最近新上一个服务，发现经常被各种爬虫和渗透测试者光顾，大概根据流水日志里user-agent看了一下主要有下面这些：

	let SPIDER_FLAG = [
	    'baiduspider',
	    '360spider',
	    'bytespider',
	    'toutiaospider',
	    'yodaobot',
	    'googlebot',
	    'teoma',
	    'msnbot',
	    'gigabot',
	    'sogou web spider',
	    'sogou inst spider',
	    'sogou spider',
	    'semrushbot',
	    'applebot',
	    'serpstatbot',
	    'zenmen', // 不知名漏洞扫描软件ZenMen_Sec V1.0
	    'python'// 主要case有Python/3.6 aiohttp/3.0.9 和 Python-urllib/2.7
	];
	
这里面既有主流的搜索引擎，也有一些漏洞扫描软件以及python爬虫脚本。对于主流的搜索引擎，可以在网站根目录放robots.txt进行约束。如下：

	# robots.txt generated by yulinyulin 
	User-agent: Baiduspider
	Disallow: /
	User-agent: Sosospider
	Disallow: /
	User-agent: sogou spider
	Disallow: /
	User-agent: YodaoBot
	Disallow: /
	User-agent: Googlebot
	Disallow: /
	User-agent: Bingbot
	Disallow: /
	User-agent: Slurp
	Disallow: /
	User-agent: Teoma
	Disallow: /
	User-agent: ia_archiver
	Disallow: /
	User-agent: twiceler
	Disallow: /
	User-agent: MSNBot
	Disallow: /
	User-agent: Scrubby
	Disallow: /
	User-agent: Robozilla
	Disallow: /
	User-agent: Gigabot
	Disallow: /
	User-agent: googlebot-image
	Disallow: /
	User-agent: googlebot-mobile
	Disallow: /
	User-agent: yahoo-mmcrawler
	Disallow: /
	User-agent: yahoo-blogs/v3.9
	Disallow: /
	User-agent: psbot

也可以在这里生成 http://tool.chinaz.com/robots/

robots协议只防君子不防小人，所以对于其它漏洞扫描软件或其他爬虫，可以自己简单处理。所以写了个中间件，简单地根据user-agent判断是否是爬虫从而进行拦截。

koa-antispider(https://www.npmjs.com/package/antispider) 和 egg-antispider(https://www.npmjs.com/package/egg-antispider)

当然，根据user-agent防范肯定不是很靠谱的，所以还可以根据ip, 频控等其它手段拦截。